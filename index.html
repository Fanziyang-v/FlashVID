<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Primary Meta Tags -->
    <!-- TODO: Replace with your paper title and author names -->
    <meta
      name="title"
      content="FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging - Ziyang Fan"
    />
    <!-- TODO: Write a compelling 150-160 character description of your research -->
    <meta
      name="description"
      content="We introduce FlashVID, a training-free and plug-and-play inference acceleration framework for Video LLMs, enabling a satisfactory speedup with negligible performance degradation."
    />
    <!-- TODO: Add 5-10 relevant keywords for your research area -->
    <meta
      name="keywords"
      content="Efficient Large Multimodal Models, Video Large Language Models, Visual Token Compression, machine learning, computer vision, AI"
    />
    <!-- TODO: List all authors -->
    <meta
      name="author"
      content="Ziyang Fan, Keyu Chen, Ruilong Xing, Yulin Li, Li Jiang, Zhuotao Tian"
    />
    <meta name="robots" content="index, follow" />
    <meta name="language" content="English" />

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article" />
    <!-- TODO: Replace with your institution or lab name -->
    <!-- <meta property="og:site_name" content="INSTITUTION_OR_LAB_NAME" /> -->
    <!-- TODO: Same as paper title above -->
    <meta
      property="og:title"
      content="FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging"
    />
    <!-- TODO: Same as description above -->
    <meta
      property="og:description"
      content="We introduce FlashVID, a training-free and plug-and-play inference acceleration framework for Video LLMs, enabling a satisfactory speedup with negligible performance degradation."
    />
    <!-- TODO: Replace with your actual website URL -->
    <!-- <meta
      property="og:url"
      content="https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
    /> -->
    <!-- TODO: Create a 1200x630px preview image and update path -->
    <meta
      property="og:image"
      content="https://github.com/Fanziyang-v/FlashVID/raw/main/assets/teaser.png"
    />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />
    <meta
      property="og:image:alt"
      content="FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging - Research Preview"
    />
    <meta
      property="article:published_time"
      content="2026-02-10T00:00:00.000Z"
    />
    <meta property="article:author" content="Ziyang Fan" />
    <meta property="article:section" content="Research" />
    <meta property="article:tag" content="Efficient Large Multimodal Models" />
    <meta property="article:tag" content="Video Large Language Models" />
    <meta property="article:tag" content="Visual Token Compression" />

    <!-- Twitter -->
    <meta
      name="twitter:card"
      content="https://github.com/Fanziyang-v/FlashVID/raw/main/assets/teaser.png"
    />
    <!-- TODO: Replace with your lab/institution Twitter handle -->
    <!-- <meta name="twitter:site" content="@YOUR_TWITTER_HANDLE" /> -->
    <!-- TODO: Replace with first author's Twitter handle -->
    <!-- <meta name="twitter:creator" content="@AUTHOR_TWITTER_HANDLE" /> -->
    <!-- TODO: Same as paper title above -->
    <meta
      name="twitter:title"
      content="FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging"
    />
    <!-- TODO: Same as description above -->
    <meta
      name="twitter:description"
      content="We introduce FlashVID, a training-free and plug-and-play inference acceleration framework for Video LLMs, enabling a satisfactory speedup with negligible performance degradation."
    />
    <!-- TODO: Same as social preview image above -->
    <meta
      name="twitter:image"
      content="https://github.com/Fanziyang-v/FlashVID/raw/main/assets/teaser.png"
    />
    <meta
      name="twitter:image:alt"
      content="FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging - Research Preview"
    />

    <!-- Academic/Research Specific -->
    <meta
      name="citation_title"
      content="FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging"
    />
    <meta name="citation_author" content="Ziyang, Fan" />
    <meta name="citation_author" content="Keyu, Chen" />
    <meta name="citation_publication_date" content="2026" />
    <meta name="citation_conference_title" content="ICLR" />
    <meta
      name="citation_pdf_url"
      content="https://openreview.net/pdf?id=H6rDX4w6Al"
    />

    <!-- Additional SEO -->
    <meta name="theme-color" content="#2563eb" />
    <meta name="msapplication-TileColor" content="#2563eb" />
    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="default" />

    <!-- Preconnect for performance -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link rel="preconnect" href="https://ajax.googleapis.com" />
    <link rel="preconnect" href="https://documentcloud.adobe.com" />
    <link rel="preconnect" href="https://cdn.jsdelivr.net" />

    <!-- TODO: Replace with your paper title and authors -->
    <title>
      FlashVID: Efficient Video Large Language Models via Training-free
      Tree-based Spatiotemporal Token Merging - Ziyang Fan | Academic Research
    </title>

    <!-- Favicon and App Icons -->
    <link rel="icon" type="image/x-icon" href="static/images/favicon.ico" />
    <link rel="apple-touch-icon" href="static/images/favicon.ico" />

    <!-- Critical CSS - Load synchronously -->
    <link rel="stylesheet" href="static/css/bulma.min.css" />
    <link rel="stylesheet" href="static/css/index.css" />

    <!-- Non-critical CSS - Load asynchronously -->
    <link
      rel="preload"
      href="static/css/bulma-carousel.min.css"
      as="style"
      onload="
        this.onload = null;
        this.rel = 'stylesheet';
      "
    />
    <link
      rel="preload"
      href="static/css/bulma-slider.min.css"
      as="style"
      onload="
        this.onload = null;
        this.rel = 'stylesheet';
      "
    />
    <link
      rel="preload"
      href="static/css/fontawesome.all.min.css"
      as="style"
      onload="
        this.onload = null;
        this.rel = 'stylesheet';
      "
    />
    <link
      rel="preload"
      href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
      as="style"
      onload="
        this.onload = null;
        this.rel = 'stylesheet';
      "
    />

    <!-- Fallback for browsers that don't support preload -->
    <noscript>
      <link rel="stylesheet" href="static/css/bulma-carousel.min.css" />
      <link rel="stylesheet" href="static/css/bulma-slider.min.css" />
      <link rel="stylesheet" href="static/css/fontawesome.all.min.css" />
      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css"
      />
    </noscript>

    <!-- Fonts - Optimized loading -->
    <link
      href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap"
      rel="stylesheet"
    />

    <!-- Defer non-critical JavaScript -->
    <script
      defer
      src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"
    ></script>
    <script
      defer
      src="https://documentcloud.adobe.com/view-sdk/main.js"
    ></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script defer src="static/js/bulma-carousel.min.js"></script>
    <script defer src="static/js/bulma-slider.min.js"></script>
    <script defer src="static/js/index.js"></script>

    <!-- Structured Data for Academic Papers -->
    <script type="application/ld+json">
      {
        "@context": "https://schema.org",
        "@type": "ScholarlyArticle",
        "headline": "PAPER_TITLE",
        "description": "BRIEF_DESCRIPTION_OF_YOUR_RESEARCH_CONTRIBUTION_AND_FINDINGS",
        "author": [
          {
            "@type": "Person",
            "name": "FIRST_AUTHOR_NAME",
            "affiliation": {
              "@type": "Organization",
              "name": "INSTITUTION_NAME"
            }
          },
          {
            "@type": "Person",
            "name": "SECOND_AUTHOR_NAME",
            "affiliation": {
              "@type": "Organization",
              "name": "INSTITUTION_NAME"
            }
          }
        ],
        "datePublished": "2024-01-01",
        "publisher": {
          "@type": "Organization",
          "name": "CONFERENCE_OR_JOURNAL_NAME"
        },
        "url": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE",
        "image": "https://YOUR_DOMAIN.com/static/images/social_preview.png",
        "keywords": [
          "KEYWORD1",
          "KEYWORD2",
          "KEYWORD3",
          "machine learning",
          "computer vision"
        ],
        "abstract": "FULL_ABSTRACT_TEXT_HERE",
        "citation": "BIBTEX_CITATION_HERE",
        "isAccessibleForFree": true,
        "license": "https://creativecommons.org/licenses/by/4.0/",
        "mainEntity": {
          "@type": "WebPage",
          "@id": "https://YOUR_DOMAIN.com/YOUR_PROJECT_PAGE"
        },
        "about": [
          {
            "@type": "Thing",
            "name": "RESEARCH_AREA_1"
          },
          {
            "@type": "Thing",
            "name": "RESEARCH_AREA_2"
          }
        ]
      }
    </script>

    <!-- Website/Organization Structured Data -->
    <script type="application/ld+json">
      {
        "@context": "https://schema.org",
        "@type": "Organization",
        "name": "INSTITUTION_OR_LAB_NAME",
        "url": "https://YOUR_INSTITUTION_WEBSITE.com",
        "logo": "https://YOUR_DOMAIN.com/static/images/favicon.ico",
        "sameAs": [
          "https://twitter.com/YOUR_TWITTER_HANDLE",
          "https://github.com/YOUR_GITHUB_USERNAME"
        ]
      }
    </script>
    <style></style>
  </head>

  <body>
    <!-- Scroll to Top Button -->
    <button
      class="scroll-to-top"
      onclick="scrollToTop()"
      title="Scroll to top"
      aria-label="Scroll to top"
    >
      <i class="fas fa-chevron-up"></i>
    </button>

    <!-- More Works Dropdown -->
    <div class="more-works-container" style="display: none">
      <button
        class="more-works-btn"
        onclick="toggleMoreWorks()"
        title="View More Works from Our Lab"
      >
        <i class="fas fa-flask"></i>
        More Works
        <i class="fas fa-chevron-down dropdown-arrow"></i>
      </button>
      <div class="more-works-dropdown" id="moreWorksDropdown">
        <div class="dropdown-header">
          <h4>More Works from Our Lab</h4>
          <button class="close-btn" onclick="toggleMoreWorks()">
            <i class="fas fa-times"></i>
          </button>
        </div>
        <div class="works-list">
          <!-- TODO: Replace with your lab's related works -->
          <a
            href="https://arxiv.org/abs/PAPER_ID_1"
            class="work-item"
            target="_blank"
          >
            <div class="work-info">
              <!-- TODO: Replace with actual paper title -->
              <h5>Paper Title 1</h5>
              <!-- TODO: Replace with brief description -->
              <p>Brief description of the work and its main contribution.</p>
              <!-- TODO: Replace with venue and year -->
              <span class="work-venue">Conference/Journal 2024</span>
            </div>
            <i class="fas fa-external-link-alt"></i>
          </a>
          <!-- TODO: Add more related works or remove extra items -->
          <a
            href="https://arxiv.org/abs/PAPER_ID_2"
            class="work-item"
            target="_blank"
          >
            <div class="work-info">
              <h5>Paper Title 2</h5>
              <p>Brief description of the work and its main contribution.</p>
              <span class="work-venue">Conference/Journal 2023</span>
            </div>
            <i class="fas fa-external-link-alt"></i>
          </a>
          <a
            href="https://arxiv.org/abs/PAPER_ID_3"
            class="work-item"
            target="_blank"
          >
            <div class="work-info">
              <h5>Paper Title 3</h5>
              <p>Brief description of the work and its main contribution.</p>
              <span class="work-venue">Conference/Journal 2023</span>
            </div>
            <i class="fas fa-external-link-alt"></i>
          </a>
        </div>
      </div>
    </div>

    <main id="main-content">
      <section class="hero">
        <div class="hero-body">
          <div class="container is-max-desktop">
            <div class="columns is-centered">
              <div class="column has-text-centered">
                <!-- TODO: Replace with your paper title -->
                <h1
                  class="title is-1 publication-title"
                  style="font-size: 2.875rem"
                >
                  FlashVID: Efficient Video Large Language Models via
                  Training-free Tree-Based Spatiotemporal Token Merging
                </h1>
                <div class="is-size-5 publication-authors">
                  <!-- TODO: Replace with your paper authors and their personal links -->
                  <span class="author-block">
                    <a href="https://github.com/Fanziyang-v" target="_blank"
                      >Ziyang Fan<sup>1</sup>,
                    </a>
                  </span>
                  <span class="author-block">
                    <a href="https://github.com/Mirei124" target="_blank"
                      >Keyu Chen<sup>1</sup>,
                    </a>
                  </span>
                  <span class="author-block">
                    <a href="https://github.com/xrlexpert" target="_blank"
                      >Ruilong Xing<sup>1</sup>,
                    </a>
                  </span>
                  <span class="author-block">
                    <a href="https://github.com/yu-lin-li" target="_blank"
                      >Yulin Li<sup>1</sup>,
                    </a>
                  </span>
                  <span class="author-block">
                    <a href="https://github.com/llijiang" target="_blank"
                      >Li Jiang<sup>2,3</sup>,
                    </a>
                  </span>
                  <span class="author-block">
                    <a href="https://github.com/tianzhuotao" target="_blank"
                      >Zhuotao Tian<sup>1,3*</sup>
                    </a>
                  </span>
                </div>

                <div class="is-size-5 publication-authors">
                  <!-- TODO: Replace with your institution and conference/journal info -->
                  <span class="author-block">
                    <sup>1</sup>Harbin Institute of Technology, Shenzhen<br />
                    <sup>2</sup>The Chinese University of Hong Kong, Shenzhen<br />
                    <sup>3</sup>Shenzhen Loop Area Institute <br />
                    <div style="position: relative">
                      <div class="iclr">ICLR 2026 Oral</div>
                    </div>
                  </span>
                  <!-- TODO: Remove this line if no equal contribution -->
                  <span class="eql-cntrb"
                    ><small
                      ><br /><sup>*</sup>Indicates Corresponding Author</small
                    ></span
                  >
                </div>

                <div class="column has-text-centered">
                  <div class="publication-links">
                    <!-- TODO: Update with your arXiv paper ID -->
                    <span class="link-block">
                      <a
                        href="https://openreview.net/forum?id=H6rDX4w6Al"
                        target="_blank"
                        class="external-link button is-normal is-rounded is-dark"
                      >
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- TODO: Add your supplementary material PDF or remove this section -->
                    <!-- <span class="link-block">
                    <a href="https://arxiv.org/abs/2602.08024" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                    <!-- TODO: Replace with your GitHub repository URL -->
                    <span class="link-block">
                      <a
                        href="https://github.com/Fanziyang-v/FlashVID"
                        target="_blank"
                        class="external-link button is-normal is-rounded is-dark"
                      >
                        <span class="icon">
                          <i class="fab fa-github"></i>
                        </span>
                        <span>Code</span>
                      </a>
                    </span>

                    <!-- TODO: Update with your arXiv paper ID -->
                    <span class="link-block">
                      <a
                        href="https://arxiv.org/abs/2602.08024"
                        target="_blank"
                        class="external-link button is-normal is-rounded is-dark"
                      >
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>arXiv</span>
                      </a>
                    </span>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </section>

      <!-- Teaser video-->
      <section class="hero teaser" style="display: none">
        <div class="container is-max-desktop">
          <div class="hero-body">
            <!-- TODO: Replace with your teaser video -->
            <video
              poster=""
              id="tree"
              autoplay
              controls
              muted
              loop
              height="100%"
              preload="metadata"
            >
              <!-- TODO: Add your video file path here -->
              <source src="static/videos/banner_video.mp4" type="video/mp4" />
            </video>
            <!-- TODO: Replace with your video description -->
            <h2 class="subtitle has-text-centered">
              Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut
              lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas
              dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut
              maximus.
            </h2>
          </div>
        </div>
      </section>
      <!-- End teaser video -->

      <section class="hero is-small">
        <div class="hero-body">
          <div class="container">
            <!-- Paper video. -->
            <h2 class="title is-3 has-text-centered">Highlights</h2>
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <img
                  src="https://github.com/Fanziyang-v/FlashVID/raw/main/assets/teaser.png"
                  alt="teaser"
                />
                <ol
                  style="
                    text-align: left;
                    margin: 1em 0 0 1em;
                    line-height: 1.75em;
                  "
                >
                  <li>
                    Our FlashVID significantly outperforms previous
                    state-of-the-art acceleration frameworks (e.g., VisionZip,
                    FastVID) across three representative VLLMs (i.e.,
                    LLaVA-OneVision, LLaVA-Video, Qwen2.5-VL) on five widely
                    used video understanding benchmarks (i.e., VideoMME,
                    EgoSchema, LongVideoBench, MVBench, MLVU).
                  </li>
                  <li>
                    FlashVID can serve as a training-free and plug-and-play
                    module for extending long video frames, enabling a 10x
                    increase in video frame input to Qwen2.5-VL, resulting in
                    8.6% within the same computational budget.
                  </li>
                  <li>
                    Existing efficient Video LLM methods often independently
                    compress spatial and temporal redundancy, overlooking the
                    intrinsic spatiotemporal relationships in videos. To address
                    this, we present a simple yet effective solution: Tree-based
                    Spatiotemporal Token Merging (TSTM) for fine-grained
                    spatiotemporal redundancy compression.
                  </li>
                </ol>
              </div>
            </div>
          </div>
        </div>
      </section>

      <!-- Paper abstract -->
      <section class="section hero is-light" style="padding-bottom: 0em">
        <div class="container is-max-desktop">
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <h2 class="title is-3">Abstract</h2>
              <div class="content has-text-justified">
                <!-- TODO: Replace with your paper abstract -->
                <p>
                  Although Video Large Language Models (VLLMs) have shown
                  remarkable capabilities in video understanding, they are
                  required to process high volumes of visual tokens, causing
                  significant computational inefficiency. Existing VLLMs
                  acceleration frameworks usually compress spatial and temporal
                  redundancy independently, which overlooks the spatiotemporal
                  relationships, thereby leading to suboptimal spatiotemporal
                  compression. The highly correlated visual features are likely
                  to change in spatial position, scale, orientation, and other
                  attributes over time due to the dynamic nature of video.
                  Building on this insight, we introduce FlashVID, a
                  training-free inference acceleration framework for VLLMs.
                  Specifically, FlashVID utilizes Attention and Diversity-based
                  Token Selection (ADTS) to select the most representative
                  tokens for basic video representation, then applies Tree-based
                  Spatiotemporal Token Merging (TSTM) for fine-grained
                  spatiotemporal redundancy elimination. Extensive experiments
                  conducted on three representative VLLMs across five video
                  understanding benchmarks demonstrate the effectiveness and
                  generalization of our method. Notably, by retaining only
                  <span className="font-bold">10%</span> of visual tokens,
                  FlashVID preserves <span className="font-bold">99.1%</span> of
                  the performance of LLaVA-OneVision. Consequently, FlashVID can
                  serve as a training-free and plug-and-play module for
                  extending long video frames, which enables a
                  <span className="font-bold">10x</span> increase in video frame
                  input to Qwen2.5-VL, resulting in a relative improvement of
                  <span>8.6%</span> within the same computational budget. Code
                  is available at
                  <a
                    style="color: var(--primary-color)"
                    href="https://github.com/Fanziyang-v/FlashVID"
                    >https://github.com/Fanziyang-v/FlashVID</a
                  >.
                </p>
              </div>
            </div>
          </div>
        </div>
      </section>
      <!-- End paper abstract -->

      <section class="hero is-small">
        <div class="hero-body">
          <div class="container">
            <!-- Paper video. -->
            <h2 class="title is-3 has-text-centered">Motivation</h2>
            <div class="columns is-centered">
              <div class="column is-four-fifths">
                <img
                  style="margin-bottom: 1em"
                  src="https://github.com/Fanziyang-v/FlashVID/raw/main/assets/motivation.png"
                  alt="motivation"
                />
                <span
                  style="
                    font-weight: bold;
                    font-size: 1.125em;
                    text-align: left;
                  "
                  >Temporal redundancy is not bound to fixed spatial
                  locations.</span
                >
                <span
                  >Semantically consistent elements in videos often shift in
                  spatial position, scale, or appearance due to motion and scene
                  dynamics, making rigid spatial correspondence across frames
                  unreliable;</span
                ><br />
                <span
                  style="
                    font-weight: bold;
                    font-size: 1.125em;
                    text-align: left;
                  "
                  >Spatial and temporal redundancy are inherently coupled.</span
                >
                <span
                  >Redundant regions within a single frame frequently persist
                  across multiple frames. Decoupled spatiotemporal redundancy
                  compression overlooks the intrinsic spatiotemporal
                  relationships, leading to suboptimal compression.</span
                >
              </div>
            </div>
          </div>
        </div>
      </section>

      <section class="hero is-small is-light">
        <div class="hero-body">
          <div class="container">
            <!-- Paper video. -->
            <h2 class="title is-3 has-text-centered">Method</h2>
            <div class="columns is-centered">
              <div class="column is-four-fifths">
                <img
                  src="https://github.com/Fanziyang-v/FlashVID/raw/main/assets/method.png"
                  alt="method"
                />
                <p style="text-align: center; margin: 0 0 1em 1em">
                  Illustration of FlashVID. We compress visual tokens by two
                  synergistic modules.
                </p>

                <ul>
                  <li>
                    <span
                      style="
                        font-weight: bold;
                        font-size: 1.125em;
                        text-align: left;
                      "
                      >Attention and Diversity-based Token Selection
                      (ADTS)</span
                    >
                    prioritizes spatiotemporally informative tokens while
                    ensuring feature diversity by solving a calibrated Max-Min
                    Diversity Problem (MMDP);
                  </li>
                  <li>
                    <span
                      style="
                        font-weight: bold;
                        font-size: 1.125em;
                        text-align: left;
                      "
                      >Tree-based Saptiotemporal Token Merging (TSTM)</span
                    >
                    models redundancy by spatiotemporal redundancy trees, which
                    effectively capture fine-grained video dynamics. Each
                    redundancy tree will be aggregated into a single token
                    representation.
                  </li>
                </ul>
              </div>
            </div>
          </div>
        </div>
      </section>

      <!-- Image carousel -->
      <section class="hero is-small" style="display: none">
        <div class="hero-body">
          <div class="container">
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <!-- TODO: Replace with your research result images -->
                <img
                  src="static/images/carousel1.jpg"
                  alt="First research result visualization"
                  loading="lazy"
                />
                <!-- TODO: Replace with description of this result -->
                <h2 class="subtitle has-text-centered">
                  First image description.
                </h2>
              </div>
              <div class="item">
                <!-- Your image here -->
                <img
                  src="static/images/carousel2.jpg"
                  alt="Second research result visualization"
                  loading="lazy"
                />
                <h2 class="subtitle has-text-centered">
                  Second image description.
                </h2>
              </div>
              <div class="item">
                <!-- Your image here -->
                <img
                  src="static/images/carousel3.jpg"
                  alt="Third research result visualization"
                  loading="lazy"
                />
                <h2 class="subtitle has-text-centered">
                  Third image description.
                </h2>
              </div>
              <div class="item">
                <!-- Your image here -->
                <img
                  src="static/images/carousel4.jpg"
                  alt="Fourth research result visualization"
                  loading="lazy"
                />
                <h2 class="subtitle has-text-centered">
                  Fourth image description.
                </h2>
              </div>
            </div>
          </div>
        </div>
      </section>
      <!-- End image carousel -->

      <!-- Youtube video -->
      <section class="hero is-small is-light" style="display: none">
        <div class="hero-body">
          <div class="container">
            <!-- Paper video. -->
            <h2 class="title is-3">Video Presentation</h2>
            <div class="columns is-centered has-text-centered">
              <div class="column is-four-fifths">
                <div class="publication-video">
                  <!-- TODO: Replace with your YouTube video ID -->
                  <iframe
                    src="https://www.youtube.com/embed/JkaxUblCGz0"
                    frameborder="0"
                    allow="autoplay; encrypted-media"
                    allowfullscreen
                  ></iframe>
                </div>
              </div>
            </div>
          </div>
        </div>
      </section>
      <!-- End youtube video -->

      <!-- Video carousel -->
      <section class="hero is-small" style="display: none">
        <div class="hero-body">
          <div class="container">
            <h2 class="title is-3">Another Carousel</h2>
            <div id="results-carousel" class="carousel results-carousel">
              <div class="item item-video1">
                <!-- TODO: Add poster image for better preview -->
                <video
                  poster=""
                  id="video1"
                  controls
                  muted
                  loop
                  height="100%"
                  preload="metadata"
                >
                  <!-- Your video file here -->
                  <source src="static/videos/carousel1.mp4" type="video/mp4" />
                </video>
              </div>
              <div class="item item-video2">
                <!-- TODO: Add poster image for better preview -->
                <video
                  poster=""
                  id="video2"
                  controls
                  muted
                  loop
                  height="100%"
                  preload="metadata"
                >
                  <!-- Your video file here -->
                  <source src="static/videos/carousel2.mp4" type="video/mp4" />
                </video>
              </div>
              <div class="item item-video3">
                <!-- TODO: Add poster image for better preview -->
                <video
                  poster=""
                  id="video3"
                  controls
                  muted
                  loop
                  height="100%"
                  preload="metadata"
                >
                  <!-- Your video file here -->
                  <source src="static/videos/carousel3.mp4" type="video/mp4" />
                </video>
              </div>
            </div>
          </div>
        </div>
      </section>
      <!-- End video carousel -->

      <!-- Paper poster -->
      <section class="hero is-small is-light" style="display: none">
        <div class="hero-body">
          <div class="container">
            <h2 class="title">Poster</h2>

            <!-- TODO: Replace with your poster PDF -->
            <iframe src="static/pdfs/sample.pdf" width="100%" height="550">
            </iframe>
          </div>
        </div>
      </section>
      <!--End paper poster -->

      <!--BibTex citation -->
      <section class="section" id="BibTeX">
        <div class="container is-max-desktop content">
          <div class="bibtex-header">
            <h2 class="title">BibTeX</h2>
            <button
              class="copy-bibtex-btn"
              onclick="copyBibTeX()"
              title="Copy BibTeX to clipboard"
            >
              <i class="fas fa-copy"></i>
              <span class="copy-text">Copy</span>
            </button>
          </div>
          <pre id="bibtex-code"><code>@inproceedings{fan2026flashvid,
  title={FlashVID: Efficient Video Large Language Models via Training-free Tree-based Spatiotemporal Token Merging},
  author={Fan, Ziyang and Chen, Keyu and Xing, Ruilong and Li, Yulin and Jiang, Li and Tian, Zhuotao},
  journal={Proceedings of the 14th International Conference on Learning Representations},
  year={2026}
}</code></pre>
        </div>
      </section>
      <!--End BibTex citation -->
    </main>

    <footer class="footer">
      <div class="container">
        <div class="columns is-centered">
          <div class="column is-8">
            <div class="content">
              <p>
                This page was built using the
                <a
                  href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                  target="_blank"
                  >Academic Project Page Template</a
                >
                which was adopted from the <a
                  href="https://nerfies.github.io"
                  target="_blank"
                  >Nerfies</a
                > project page. You are free to borrow the source code of this
                website, we just ask that you link back to this page in the
                footer. <br />
                This website is licensed under a
                <a
                  rel="license"
                  href="http://creativecommons.org/licenses/by-sa/4.0/"
                  target="_blank"
                  >Creative Commons Attribution-ShareAlike 4.0 International
                  License</a
                >.
              </p>
            </div>
          </div>
        </div>
      </div>
    </footer>

    <!-- Statcounter tracking code -->

    <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->
  </body>
</html>
